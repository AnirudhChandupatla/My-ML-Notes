{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjNtzlpNuyjD"
   },
   "source": [
    "##*K/N-fold Cross Validation:*\n",
    "\n",
    "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/General_Miscellaneous/pictures/k-fold.png?raw=true)\n",
    "\n",
    "-> This method can be used for almost all ML algorithms.\n",
    "\n",
    "-> Follow the above procedure for a single Hyper Parameter(HP) amongst a set.\n",
    "\n",
    "-> We can keep 90% / 95% of Data set( D ) for training and 10% or 5% for testing if we do this manually \n",
    "\n",
    "-> lets say we have 1000 data_pts and we'd trained a model on 90% of D i.e 900 data_pts then can't we get accuracy score using remaining 100 data_pts.\n",
    "\n",
    "Like,\n",
    "\n",
    "    intermediate_metrics = [] \n",
    "\n",
    "    for HP in set_of_HP's:\n",
    "\n",
    "        Fold_metrics = []  \n",
    "\n",
    "        while i < N:\n",
    "\n",
    "           Fold i : Cross validate using HP --->> get metric-i\n",
    "           \n",
    "           append metric-i to Fold_metrics\n",
    "           \n",
    "           increment i\n",
    "           \n",
    "        append {avg(Fold_metrics) , HP} to intermediare_metrics\n",
    "        \n",
    "    Finally choose the best required Hyper Parameter(HP) from intermediate_metrics based on avg(Fold_metrics)--This could be Accuracy or log-loss or etc.\n",
    "    \n",
    "    Easiest way is to plot a graph between HP's and avg(Fold_metrics) since we get them as pairs. \n",
    "    \n",
    "-> Do keep number of folds i.e N always less than 10, even for 10 it takes much time. \n",
    "\n",
    "-> There is a way around it by using **Intel's Distribution for Python** since it's significantly faster than traditional python and you can get all the necessary ML libraries in it which is not the case with Pypy. \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZHjpypFwlow"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "-> **D-train **, is to **fit a mathematical function or algorithm** on our training data.\n",
    "\n",
    "-> **D-cross_validate** , is for **hyperparameter tunning**.\n",
    "\n",
    "-> **D-test** , is for getting **accuracy score**.\n",
    "\n",
    "and where D-train + D-test + D-cross_validate = Dn(i.e all the available data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obiEMEAu7Sbx"
   },
   "source": [
    "# *Handling imbalanced Data:*\n",
    "[ref](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "General-Miscellaneous.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
