{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/Naive_Bayes.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "7sbKxkjgzAif",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-> The name **'Bayes'** comes from Baye's Theorem\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/bth.png?raw=true)\n",
        "\n",
        "-> By applying **conditional probality** rule the numerator part can be written as ,\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/sumr.png?raw=true)\n",
        "\n",
        "-> Solving above equation is tedious and will lead to inaccurate results , so **Naive** assumption(i.e features are **conditionally independent** of each other) is needed. \n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/nvas.png?raw=true)\n",
        "\n",
        "-> With naive assumption first eq is reduced to,\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/redc.png?raw=true)\n",
        "\n",
        "-> From first eq ''p(x)'' is constant so avoid it, above eq is know as **probability model** now to make it a classifier we'll add a rule known as MAP(maximum a posteriori)\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/mdl.png?raw=true)\n",
        "\n",
        "-> Finally this is our **Naive Baye's Classifer**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V-s3sRHegPZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Various Foms of Naive Bayes:***\n",
        "\n",
        "**Gaussian**:\n",
        " \n",
        "    -> Assumes that features follow a normal distribution, \n",
        "    and performs very nicely if data is infact normally distributed.\n",
        "    Much generally used method.\n",
        "    \n",
        "**Bernoulli:**\n",
        "\n",
        "    -> Used for binary features, for example in classifying text with \n",
        "    binary bag of words featurization.\n",
        "    \n",
        "**Multinomial:**  \n",
        "\n",
        "    -> Used for count based features.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dsRf67QFomWE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Improvising Naive Bayes:***\n",
        "\n",
        "    -> Convert features to normal distributiion if not already \n",
        "    for better performence of GaussionNB.\n",
        "    \n",
        "    -> Remove Correlated features."
      ]
    },
    {
      "metadata": {
        "id": "fVw_1nDrkT9i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Use Cases:***\n",
        "\n",
        "    -> Generally used as baseline model to compare other models.\n",
        "\n",
        "    -> On text and categorical data."
      ]
    },
    {
      "metadata": {
        "id": "4LaO4Oul9qIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***pros:***\n",
        "\n",
        "    -> Simple and fast.\n",
        "    \n",
        "    -> Works well even with less data.\n",
        "    \n",
        "    -> Easily interpretable\n",
        "    \n",
        "    -> Can handle high dimensional data better than some complex models , as data points are well seperated there."
      ]
    },
    {
      "metadata": {
        "id": "9IZ4D3IBecWZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***some good references:***\n",
        "\n",
        "[Ref 1](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
        "\n",
        "[Ref 2](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html)\n",
        "\n",
        "[Ref 3](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\n",
        "\n",
        "[Ref 4](https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html)\n",
        "\n",
        "[Bit confusing one](https://medium.com/@akankshamalhotra24/naive-bayes-theorem-79832d506a63)\n",
        "\n",
        "[Probabilistic Derivation](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Probabilistic_model)\n",
        "\n",
        "[extra](https://towardsdatascience.com/naive-bayes-in-machine-learning-f49cc8f831b4)"
      ]
    }
  ]
}