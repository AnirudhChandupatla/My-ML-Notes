{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "7sbKxkjgzAif",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The name **'Bayes'** comes from Baye's Theorem\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/bth.png?raw=true)\n",
        "\n",
        "By applying **conditional probality** rule the numerator part can be written as ,\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/sumr.png?raw=true)\n",
        "\n",
        "Solving above equation is tedious and will lead to inaccurate results , so **Naive** assumption(i.e features are **conditionally independent** of each other) is needed. \n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/nvas.png?raw=true)\n",
        "\n",
        "With naive assumption first eq is reduced to,\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/redc.png?raw=true)\n",
        "\n",
        "From first eq ''p(x)'' is constant so avoid it, above eq is know as **probability model** now to make it a classifier we'll add a rule known as MAP(maximum a posteriori)\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/mdl.png?raw=true)\n",
        "\n",
        "Finally this is our **Naive Baye's Classifer**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tN9rlYCkOYig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Additive Smoothing:***\n",
        "\n",
        "There will be **atleast 1% chance to find some new words during testing or in production which aren't in D<sub>train</sub>**, \n",
        "\n",
        "then **p( that_new_word | C<sub>k</sub>) = 0** and inturn the entire NBclassifier becomes zero which we don't want to happen.\n",
        "\n",
        "A solution to this is additive smoothing, here we add some quantity to numerator and denominator\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/adt.png?raw=true) \n",
        "\n",
        "where, \n",
        "\n",
        "'**alpha**' is our **hyperparameter**, by keeping it high relatively to size of D<sub>train</sub> results in a near 50:50 probability for that X<sub>new</sub>  \n",
        "\n",
        "**k** is **no.of possible states an object can have**, here for object X<sub>new</sub>(i.e a word) its 2(present , not present) for any other > 2\n"
      ]
    },
    {
      "metadata": {
        "id": "3H9rqiPD66nu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Log Probabilities:***\n",
        "\n",
        "This term below gives a value which exceeds decimal precision that python can handle, like say some_val x 10<sup>-12</sup> its **almost zero and wouldn't make any sense** and once decimal position exceeds 16 digits we'll face **numerical stability**\n",
        "\n",
        "![alt text](https://github.com/anirudh991/My-ML-Notes/blob/master/Naive-Bayes/images/lgp.png?raw=true)\n",
        "\n",
        "by taking log we can avoid it , Ï€ log(p( x<sub>i</sub> | C<sub>k</sub> ))\n",
        "\n",
        "scikit learn implements a similar thing, predict_log_proba(X) which returns log-probability estimates for the test vector X."
      ]
    },
    {
      "metadata": {
        "id": "V-s3sRHegPZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Various Foms of Naive Bayes:***\n",
        "\n",
        "**Gaussian**:\n",
        " \n",
        "    Assumes that features follow a normal distribution, \n",
        "    and performs very nicely if data is infact normally distributed.\n",
        "    preferable for Numerical data.\n",
        "    \n",
        "**Bernoulli:**\n",
        "\n",
        "    Used for binary features, for example in classifying text with \n",
        "    binary bag of words featurization.\n",
        "    \n",
        "    BernoulliNB tends to performs better on \n",
        "    shorter documents / sentences.\n",
        "    \n",
        "**Multinomial:**  \n",
        "\n",
        "    Used for count based features."
      ]
    },
    {
      "metadata": {
        "id": "dsRf67QFomWE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Improvising Naive Bayes:***\n",
        "\n",
        "    Convert features to normal distributiion if not already \n",
        "    for better performence of GaussionNB.\n",
        "    \n",
        "    Remove Correlated features."
      ]
    },
    {
      "metadata": {
        "id": "fVw_1nDrkT9i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***Use Cases:***\n",
        "\n",
        "    Generally used as baseline model to compare other models.\n",
        "\n",
        "    On text and categorical data."
      ]
    },
    {
      "metadata": {
        "id": "4LaO4Oul9qIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***pros:***\n",
        "\n",
        "    Simple and fast.\n",
        "    \n",
        "    Works well even with less data.\n",
        "    \n",
        "    Easily interpretable\n",
        "    \n",
        "    Can handle high dimensional data better than some complex models , as data points are well seperated there."
      ]
    },
    {
      "metadata": {
        "id": "9IZ4D3IBecWZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ***some good references:***\n",
        "\n",
        "[Ref 1](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
        "\n",
        "[Ref 2](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html)\n",
        "\n",
        "[Ref 3](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\n",
        "\n",
        "[Ref 4](https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html)\n",
        "\n",
        "[Bit confusing one](https://medium.com/@akankshamalhotra24/naive-bayes-theorem-79832d506a63)\n",
        "\n",
        "[extra](https://towardsdatascience.com/naive-bayes-in-machine-learning-f49cc8f831b4)"
      ]
    }
  ]
}